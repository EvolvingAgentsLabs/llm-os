# LLM OS - Evolution of llmunix

> A Self-Modifying LLM Operating System with Hybrid Architecture

**Current Version**: 3.2.0 (Hybrid Architecture)
- âœ… **Phase 3.2**: Hybrid Architecture - Markdown agents + Python kernel
- âœ… Phase 3.0: HOPE - Self-modifying kernel with crystallization
- âœ… Phase 2.5: SDK hooks, streaming, nested learning
- âœ… Phase 2: Multi-agent orchestration, project management
- âœ… Phase 1: Learner-Follower pattern (cost optimization)

## ğŸŒŸ The Hybrid Architecture

**The future of LLM OS**: Agents are defined in **Markdown files** that the system can create and modify on the fly!

### Three-Layer Stack

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Markdown Mind (Cognitive Layer)       â”‚
â”‚   workspace/agents/*.md                 â”‚
â”‚   - Self-modifiable by the LLM          â”‚
â”‚   - Hot-reloadable (no restart)         â”‚
â”‚   - Human-readable, version-controllableâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Python Kernel (Somatic Layer)         â”‚
â”‚   llmos/                                â”‚
â”‚   - Type-safe, performant               â”‚
â”‚   - Security hooks, token economy       â”‚
â”‚   - Production-ready runtime            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Crystallized Intelligence (HOPE)      â”‚
â”‚   llmos/plugins/generated/              â”‚
â”‚   - Auto-generated Python tools         â”‚
â”‚   - Instant, zero-cost execution        â”‚
â”‚   - System self-optimization            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Innovation: Self-Modification

The system can **create new agents** by writing Markdown files:

```python
# System uses create_agent tool
await os.execute("Create a haiku-poet agent that writes beautiful haikus")

# Result: workspace/agents/haiku-poet.md is created
# Agent is immediately available, no restart needed!
```

**Why This Matters:**
- ğŸ“ **Just-in-Time Agents**: System creates specialists on demand
- ğŸ”„ **Hot Reload**: Changes take effect instantly
- ğŸ§  **LLM-Friendly**: System can read/write its own definitions
- ğŸ¯ **Self-Evolution**: System improves itself over time
- ğŸ“š **Version Control**: Track agent evolution in git

See **[HYBRID_ARCHITECTURE.md](HYBRID_ARCHITECTURE.md)** for full documentation.

## Overview

This repository contains **llmos** (LLM OS), the evolved version of [llmunix](../llmunix) based on the Claude Agent SDK. Starting with v3.2.0, llmos implements a **Hybrid Architecture** combining llmunix's markdown philosophy (flexibility, self-modification) with llmos's Python kernel (stability, security, performance).

## Project Structure

```
llm-os/
â”œâ”€â”€ llmos/                          # Python Kernel (Somatic Layer)
â”‚   â”œâ”€â”€ boot.py                     # Entry point
â”‚   â”œâ”€â”€ kernel/                     # Core OS components
â”‚   â”‚   â”œâ”€â”€ agent_loader.py         # ğŸ†• Markdown â†’ Runtime bridge
â”‚   â”‚   â””â”€â”€ ...                     # Scheduler, Watchdog, Event Bus
â”‚   â”œâ”€â”€ memory/                     # Storage layer (Traces, Memory)
â”‚   â”œâ”€â”€ interfaces/                 # Execution layer (Dispatcher, Orchestrator)
â”‚   â””â”€â”€ plugins/                    # Tools
â”‚       â”œâ”€â”€ system_tools.py         # ğŸ†• create_agent, list_agents, modify_agent
â”‚       â””â”€â”€ generated/              # ğŸ†• Auto-generated crystallized tools
â”‚
â”œâ”€â”€ workspace/                      # ğŸ†• Markdown Mind (Cognitive Layer)
â”‚   â””â”€â”€ agents/                     # ğŸ†• Agent definitions (.md files)
â”‚       â”œâ”€â”€ researcher.md           # Sample: Web research specialist
â”‚       â”œâ”€â”€ coder.md                # Sample: Expert coder
â”‚       â””â”€â”€ data-analyst.md         # Sample: Data analysis specialist
â”‚
â”œâ”€â”€ examples/                       # Production-ready examples
â”‚   â”œâ”€â”€ hybrid_architecture_demo.py # â­ NEW: Self-modification demo
â”‚   â”œâ”€â”€ qiskit_studio_backend/      # Quantum computing backend
â”‚   â”œâ”€â”€ q-kids-studio/              # Educational quantum (ages 8-12)
â”‚   â”œâ”€â”€ robo-os/                    # Robot control with LLM brain
â”‚   â”œâ”€â”€ demo-app/                   # Rich TUI with 7 scenarios
â”‚   â””â”€â”€ multi_agent_example.py      # Phase 2/2.5 feature showcase
â”‚
â””â”€â”€ HYBRID_ARCHITECTURE.md          # ğŸ†• Full documentation (531 lines)
```

## Key Differences

| Feature | llmunix | llmos (Phase 2.5) |
|---------|---------|-------|
| **Foundation** | Custom markdown framework | Claude Agent SDK |
| **Architecture** | Agent-based orchestration | Kernel-Cortex-Memory OS |
| **Execution** | Multi-agent pipelines | 3-mode dispatch (Learner/Follower/Orchestrator) |
| **Memory** | File-based logs | SDK-aligned traces (Markdown) + file-based storage |
| **Extensibility** | Markdown agents/tools | Python plugins + AgentDefinitions |
| **Token Management** | Implicit cost tracking | Explicit TokenEconomy + SDK hooks |
| **Focus** | Project-based CLI/mobile | Generic OS for any domain |
| **Philosophy** | Markdown-driven | CPU analogy (LLM as processor) |
| **Control Flow** | Linear | Event-driven with hooks (budget, security) |
| **Multi-Agent** | Markdown orchestration | AgentDefinition + shared SDK client |
| **Security** | N/A | PreToolUse hooks (dangerous command blocking) |
| **Streaming** | N/A | Real-time feedback with partial messages |

## Quick Start

See [llmos/GETTING_STARTED.md](llmos/GETTING_STARTED.md) for detailed instructions.

```bash
cd llmos
pip install -r requirements.txt
export ANTHROPIC_API_KEY="your-key"
python boot.py interactive
```

## Documentation

### Core Documentation
- **[llmos/README.md](llmos/README.md)** - LLM OS overview and usage
- **[llmos/ARCHITECTURE.md](llmos/ARCHITECTURE.md)** - System architecture design
- **[llmos/GETTING_STARTED.md](llmos/GETTING_STARTED.md)** - Installation and first steps
- **[llmos/DEPLOYMENT_CHECKLIST.md](llmos/DEPLOYMENT_CHECKLIST.md)** - Implementation checklist

### Examples
- **[examples/README.md](examples/README.md)** - Examples overview and navigation guide
- **[examples/qiskit_studio_backend/](examples/qiskit_studio_backend/)** - Flagship: Quantum computing backend
  - Drop-in replacement for [Qiskit Studio](https://github.com/AI4quantum/qiskit-studio) backend
  - Demonstrates production-grade LLM OS architecture
  - 100% cost savings on repeated tasks via Learnerâ†’Follower
- **[examples/q-kids-studio/](examples/q-kids-studio/)** - Educational quantum platform (ages 8-12)
  - Block-based quantum programming with gamification
  - Multi-layer safety for kids
  - 99%+ cost savings via Learnerâ†’Follower caching
- **[examples/robo-os/](examples/robo-os/)** - Robot control with LLM as brain
  - Natural language robot control
  - Multi-layer safety hooks (PreToolUse validation)
  - Operator + Safety Officer multi-agent coordination
- **[examples/demo-app/](examples/demo-app/)** - Rich terminal UI with 7 demo scenarios
- **[examples/multi_agent_example.py](examples/multi_agent_example.py)** - 12 interactive examples of Phase 2/2.5 features

## Architecture Highlights

### LLM as CPU

llmos treats the LLM as a **Central Processing Unit**:

- **Python Kernel**: Motherboard (I/O, scheduling, monitoring, hooks)
- **LLM**: Processor (planning, reasoning, learning, orchestration)
- **Tokens**: Battery (energy for cognitive cycles, controlled by hooks)

### Five Execution Modes (v3.2.0)

**1. CRYSTALLIZED Mode** (Instant & Free) - ğŸ†• HOPE Phase 3.0
```
Frequent Task: "Create API endpoint"
  â†’ Pattern used 5+ times (95%+ success)
  â†’ Execute auto-generated Python tool
  â†’ Cost: $0.00, Time: <1s
```

**2. FOLLOWER Mode** (Fast & Free)
```
Repeat Task: "Create Python calculator"
  â†’ Finds matching trace (confidence > 0.9)
  â†’ Pure Python execution
  â†’ Cost: $0.00, Time: 2-5s
```

**3. MIXED Mode** (Guided & Efficient) - Phase 2.5
```
Similar Task: "Create calculator with GUI"
  â†’ Found similar trace (confidence 0.75-0.92)
  â†’ Few-shot LLM guidance
  â†’ Cost: ~$0.25, Time: 5-15s
```

**4. LEARNER Mode** (Novel & Controlled)
```
New Task: "Create Python calculator"
  â†’ No trace found
  â†’ Claude SDK with hooks (budget, security, tracing)
  â†’ Saves execution trace (Markdown)
  â†’ Cost: ~$0.50, Time: 10-30s
```

**5. ORCHESTRATOR Mode** (Complex & Multi-Agent) - Phase 2
```
Complex Task: "Research AI trends and write report"
  â†’ Detects complexity (keywords: "and", "research")
  â†’ Breaks down into subtasks
  â†’ Creates/selects agents (researcher, writer)
  â†’ Coordinates via AgentDefinitions
  â†’ Cost: Variable (~$1-2)
```

### Token Economy

Explicit budget management:

```python
economy = TokenEconomy(budget_usd=10.0)
economy.check_budget(0.50)  # Learner Mode
economy.deduct(0.45, "Learn: Create script")
```

### Memory Hierarchy (SDK-Aligned)

- **L1**: Context window (in LLM)
- **L2**: Short-term memory (session logs)
- **L3**: Procedural memory (execution traces - **Markdown**)
- **L4**: Semantic memory (facts, insights - **file-based**)

**Phase 2.5 Update**: Memory now uses SDK-aligned structure with Markdown traces instead of YAML.

### Phase 2.5 Highlights

**SDK Hooks System** (Automatic):
- ğŸ”’ **Security Hook**: Blocks dangerous commands (`rm -rf /`, `curl | bash`)
- ğŸ’° **Budget Hook**: Prevents runaway costs, estimates before execution
- ğŸ“ **Trace Hook**: Automatic execution trace capture
- ğŸ’µ **Cost Hook**: Real-time cost monitoring
- ğŸ§  **Memory Hook**: Injects relevant past experiences

**Streaming Support**:
- Real-time progress updates
- Partial message streaming
- Non-blocking execution feedback

**Advanced SDK Integration**:
- System prompt presets (leverage Claude's optimized prompts)
- Full ClaudeAgentOptions support (model, max_turns, env, etc.)
- AgentDefinition support for multi-agent orchestration
- Shared SDK client for efficient agent coordination

## Examples & Use Cases

### ğŸŒŸ Flagship Example: Qiskit Studio Backend

**LLM OS as a drop-in replacement for complex microservice architectures**

We've reimplemented the [Qiskit Studio](https://github.com/AI4quantum/qiskit-studio) backend using LLM OS, replacing 3 separate Maestro-orchestrated microservices with a single unified backend:

**Original Architecture (Maestro):**
- `chat-agent` - RAG-based Q&A (separate microservice)
- `codegen-agent` - Quantum code generation (separate microservice)
- `coderun-agent` - Code execution (separate microservice)

**LLM OS Architecture:**
- **Quantum Tutor** agent - Chat & education (ORCHESTRATOR mode)
- **Quantum Architect** agent - Code generation (LEARNER/FOLLOWER modes)
- **Qiskit Tools** plugin - Secure code execution (Somatic Layer)

**Key Improvements:**
- ğŸ’° **100% cost savings** on repeated tasks (Learner â†’ Follower caching)
- ğŸ”’ **Enhanced security** with multi-layer protection hooks
- ğŸ§  **Unified memory** across all interactions (L4 semantic memory)
- âš¡ **90% simpler deployment** (single process vs. Docker Compose)
- ğŸ¨ **API compatible** with existing Next.js frontend

**Try it:**
```bash
cd examples/qiskit_studio_backend
./run.sh
# Backend runs on http://localhost:8000
# Compatible with Qiskit Studio frontend
```

See [examples/qiskit_studio_backend/README.md](examples/qiskit_studio_backend/README.md) for full documentation.

---

### Other Use Cases

#### Code Generation
```
llmos> Create a REST API with FastAPI
# First time: $0.50 (Learner)
# Repeat: $0 (Follower)
```

#### Data Processing
```
llmos> Parse CSV files and create summary
# Pattern saved, reusable
```

#### Research
```
llmos> Summarize latest AI papers
# Learns research pattern
```

#### Quantum Computing
```
llmos> Create a 3-qubit GHZ state circuit
# First time: $0.05 (Learner)
# Second time: $0.00 (Follower - cached!)
```

#### Robotics Control
```
llmos> Move the robot arm 30cm to the right
# Operator Agent â†’ move_relative(dx=0.3)
# Safety Hook validates position
# Robot executes safely
```

## Evolution from llmunix

llmos **evolves** from llmunix by:

1. **Adopting Claude Agent SDK** as the foundation (proper integration)
2. **Implementing OS-like architecture** (Kernel-Cortex-Memory)
3. **Adding three execution modes** (Learner/Follower/Orchestrator)
4. **Explicit token economy** with SDK hooks for control
5. **Using execution traces** as "compiled bytecode" (Markdown format)
6. **Plugin-based extensibility** + AgentDefinition support
7. **Generic design** applicable to any domain
8. **Hook-based security** and budget control (Phase 2.5)
9. **Multi-agent orchestration** with natural language delegation (Phase 2)
10. **Streaming support** for real-time feedback (Phase 2.5)

## When to Use Each

**Use llmunix when**:
- You need project-based organization with markdown configs
- Markdown-driven workflow is preferred
- Mobile app generation is needed
- You want Claude Code-native integration out of the box

**Use llmos when**:
- You need a **generic LLM operating system** with three execution modes
- **Cost optimization is critical** (hooks prevent runaway costs)
- **Security is important** (dangerous command blocking)
- You want to build up a **trace library** (Markdown format)
- **Multi-agent orchestration** is needed (AgentDefinition support)
- Plugin-based extensibility is preferred
- You're building custom tooling on **Claude Agent SDK**
- You need **streaming** for real-time feedback
- You want **proper SDK integration** with hooks
- **Replacing microservice architectures** with a unified backend (see [Qiskit Studio example](examples/qiskit_studio_backend/))

## License

Apache 2.0

---

**llmunix**: Markdown Operating System for agentic workflows
**llmos** (v3.2.0): Self-Modifying LLM Operating System with:
- **Hybrid Architecture**: Markdown Mind + Python Kernel + Crystallization
- Five execution modes (CRYSTALLIZED/FOLLOWER/MIXED/LEARNER/ORCHESTRATOR)
- Self-modification: System creates and evolves its own agents
- Hot-reload: Changes take effect instantly
- SDK hooks (budget, security, tracing)
- Multi-agent orchestration
- Token economy

Both are part of the Evolving Agents Labs ecosystem.

**Latest**: v3.2.0 introduces the **Hybrid Architecture**, combining llmunix's markdown flexibility with llmos's Python stability. The system can now create and modify agents by writing Markdown files, achieving true self-modification. See **[HYBRID_ARCHITECTURE.md](HYBRID_ARCHITECTURE.md)** for details.
