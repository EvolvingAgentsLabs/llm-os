# LLM OS Demo Application (Phase 2.5+)

A comprehensive demonstration application showcasing all capabilities of the LLM OS (llmos) - Phase 2.5+ with **Nested Learning**.

**NEW: Semantic Trace Matching with MIXED Mode Execution!**

## Overview

This demo application demonstrates the full power of llmos through practical, real-world scenarios including:

1. **ğŸ§¬ Nested Learning (NEW!)**: Semantic trace matching with LLM-based similarity analysis
2. **Three+ Execution Modes**: Learner, Follower, MIXED (NEW!), and Orchestrator
3. **Intelligent Mode Selection**: Confidence-based routing (FOLLOWER/MIXED/LEARNER)
4. **Project Management**: Organizing work into isolated projects
5. **Multi-Agent Orchestration**: Coordinating specialized agents for complex tasks
6. **Dynamic Agent Creation**: Creating agents on-demand for specific needs
7. **Memory System**: Leveraging execution traces and cross-project learning
8. **SDK Hooks**: Budget control, security, and trace capture
9. **Streaming Support**: Real-time feedback during execution
10. **Cost Optimization**: Learning once, executing infinitely at zero cost

## Quick Start

### Prerequisites

- Python 3.11+
- Claude Agent SDK installed
- ANTHROPIC_API_KEY set

### Installation

```bash
# Install dependencies
cd demo-app
pip install -r requirements.txt

# Set your API key
export ANTHROPIC_API_KEY="your-key-here"
```

### Run the Demo

```bash
# Interactive demo menu
python demo_main.py

# Run specific scenario
python demo_main.py --scenario data-pipeline

# Run all scenarios
python demo_main.py --all
```

## Demo Scenarios

### ğŸŒŸ 1. Nested Learning Demo (NEW!)
**Scenario**: Demonstrate semantic trace matching and intelligent mode selection
- **Mode**: Learner â†’ Follower â†’ MIXED
- **Features**: LLM-based similarity analysis, confidence scoring, three-mode execution
- **File**: `scenarios/nested_learning_demo.py`
- **What it shows**:
  - Initial learning (LEARNER mode)
  - Exact match replay (FOLLOWER mode - $0 cost)
  - Semantic match (similar but different) â†’ FOLLOWER/MIXED mode
  - Related task (different details) â†’ MIXED mode ($0.25 cost)
  - Unrelated task â†’ LEARNER mode
  - Automatic mode selection based on confidence scores

**Key Innovation**: The LLM analyzes semantic similarity between goals and traces, not just exact text matching. This means "create a file" and "create a file named X" are understood as semantically equivalent!

### 2. Data Processing Pipeline
**Scenario**: Build a complete data processing pipeline
- **Mode**: Orchestrator
- **Agents**: Data Collector, Data Processor, Report Generator
- **Features**: Multi-agent coordination, project management, trace capture
- **File**: (inline in demo_main.py)

### 3. Code Generation Workflow
**Scenario**: Generate, test, and document code
- **Mode**: Learner â†’ Follower
- **Features**: Trace learning, cost optimization, memory reuse
- **File**: (inline in demo_main.py)

### 4. Cost Optimization Demo
**Scenario**: Demonstrate dramatic cost savings through trace reuse
- **Mode**: Learner â†’ Follower (5 iterations)
- **Features**: Cost tracking, savings analysis
- **File**: (inline in demo_main.py)

### 5. Research Assistant (Deprecated)
**Scenario**: Research a topic and create a comprehensive report
- **Mode**: Orchestrator
- **Agents**: Research Agent, Technical Writer
- **Features**: Multi-step orchestration, dynamic agent creation
- **Status**: âš ï¸ Partially working - demonstrates multi-agent setup but has delegation timeout issues
- **Known Issues**: Some delegations timeout (300s), WebSearch may not work in delegated agents, execution takes 10-16 minutes
- **Note**: This scenario has been de-prioritized in the menu due to reliability issues. Use Data Pipeline for multi-agent demonstration.

### 6. DevOps Automation
**Scenario**: Automate deployment and monitoring tasks
- **Mode**: All three modes
- **Features**: Security hooks, budget control, follower mode efficiency
- **File**: (inline in demo_main.py)

### 7. Cross-Project Learning
**Scenario**: Demonstrate learning insights across multiple projects
- **Mode**: All modes
- **Features**: Cross-project patterns, reusable agents, optimization
- **File**: (inline in demo_main.py)

### 8. SDK Hooks Demo
**Scenario**: Demonstrate all Phase 2.5 SDK hooks
- **Mode**: Learner with hooks
- **Features**: Security, budget control, trace capture, memory injection
- **File**: (inline in demo_main.py)

## Architecture

```
demo-app/
â”œâ”€â”€ README.md                   # This file
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ demo_main.py               # Main entry point with interactive menu
â”œâ”€â”€ scenarios/                 # Demo scenarios
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_pipeline.py       # Data processing pipeline demo
â”‚   â”œâ”€â”€ code_generation.py     # Code generation with trace learning
â”‚   â”œâ”€â”€ research_assistant.py  # Research and reporting demo
â”‚   â”œâ”€â”€ devops_automation.py   # DevOps task automation
â”‚   â””â”€â”€ cross_project_demo.py  # Cross-project learning demo
â”œâ”€â”€ agents/                    # Custom agent definitions
â”‚   â”œâ”€â”€ data_collector.py      # Collects data from sources
â”‚   â”œâ”€â”€ data_processor.py      # Processes and transforms data
â”‚   â”œâ”€â”€ report_generator.py    # Generates reports
â”‚   â”œâ”€â”€ researcher.py          # Research specialist
â”‚   â””â”€â”€ technical_writer.py    # Technical documentation writer
â”œâ”€â”€ utils/                     # Utilities
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ demo_helpers.py        # Helper functions for demos
â”‚   â””â”€â”€ visualization.py       # Visualization utilities
â””â”€â”€ output/                    # Demo outputs (generated at runtime)
    â”œâ”€â”€ projects/              # Project outputs
    â”œâ”€â”€ reports/               # Generated reports
    â””â”€â”€ traces/                # Execution traces
```

## Key Features Demonstrated

### 1. Four Execution Modes (Including NEW MIXED Mode!)

**Learner Mode** - First time execution
```python
result = await os.execute("Create a Python script for data analysis")
# Cost: ~$0.50, learns pattern, creates trace
# Confidence: N/A (initial learning)
```

**Follower Mode** - Exact/near-exact match
```python
result = await os.execute("Create a Python script for data analysis")
# Cost: $0.00, replays learned trace
# Confidence: â‰¥92% (virtually identical)
```

**MIXED Mode** - Similar but different task (NEW! ğŸŒŸ)
```python
result = await os.execute("Create a Python script for customer analysis")
# Cost: ~$0.25, uses trace as few-shot guidance
# Confidence: 75-92% (similar, needs adaptation)
# Uses existing trace to guide LLM, cheaper than full LEARNER
```

**Orchestrator Mode** - Complex multi-step tasks
```python
result = await os.execute(
    "Research AI trends and create a technical report",
    mode="ORCHESTRATOR"
)
# Cost: Variable, coordinates multiple agents
```

**The Innovation**: Automatic mode selection based on LLM-analyzed semantic similarity!

### 2. Project Management

```python
# Create isolated project
project = os.create_project("data_analysis", "Analyze customer data")

# Execute in project context
await os.execute(
    "Analyze sales trends",
    project_name="data_analysis"
)

# Project has isolated:
# - Memory traces
# - Agent definitions
# - Output files
# - Execution state
```

### 3. Dynamic Agent Creation

```python
# Create specialized agent on-demand
analyst = os.create_agent(
    name="data-analyst-agent",
    category="data_analysis",
    description="Analyzes CSV data files",
    system_prompt="You are a data analysis specialist...",
    tools=["Read", "Write", "Bash"],
    capabilities=["CSV analysis", "Statistics"],
    constraints=["Max 100MB files"]
)

# Agent is automatically registered and available
```

### 4. Multi-Agent Orchestration

```python
# Orchestrator coordinates multiple agents
result = await os.execute(
    "Collect customer data, process it, and generate report",
    mode="ORCHESTRATOR"
)

# Behind the scenes:
# 1. Creates/selects: DataCollector, DataProcessor, ReportGenerator
# 2. Registers them as AgentDefinitions in SDK
# 3. Coordinates execution via natural language delegation
# 4. Synthesizes final output
```

### 5. SDK Hooks (Phase 2.5)

**Automatic Hook Integration**:
- Budget Control: Prevents runaway costs
- Security: Blocks dangerous commands
- Trace Capture: Records execution for Follower mode
- Cost Tracking: Monitors cumulative costs
- Memory Injection: Provides context from past executions

```python
# Hooks are enabled automatically in Learner mode
result = await os.execute(
    "Create a script",
    max_cost_usd=1.0  # Budget hook ensures cost < limit
)
```

### 6. Cross-Project Learning

```python
# Get common patterns across projects
patterns = await os.get_cross_project_insights()

# Identify reusable agents
reusable = await os.get_reusable_agents()

# Get project-specific summary
summary = await os.get_project_summary("my_project")
```

### 7. Streaming Support

```python
# Define streaming callback
async def on_stream(event):
    print(".", end="", flush=True)

# Execute with real-time feedback
result = await os.sdk_client.execute_learner_mode(
    goal="Write a long document",
    enable_streaming=True,
    streaming_callback=on_stream
)
```

## Cost Analysis

The demo tracks costs across scenarios:

| Scenario | First Run (Learner) | Repeat (Follower) | Savings | Status |
|----------|---------------------|-------------------|---------|--------|
| Simple Code Gen | $0.50 | $0.00 | 100% | âœ… Working |
| Data Pipeline | $1.20 | $0.00 | 100% | âœ… Working |
| Research Task | $0.30-0.50 | N/A | N/A | âš ï¸ Timeouts |
| DevOps Task | $0.30 | $0.00 | 100% | âœ… Working |
| Cost Optimization | $0.50 | $0.00 | 100% | âœ… Working |
| SDK Hooks | $0.30 | $0.00 | 100% | âœ… Working |

**Total savings after trace library built**: ~90-100% (for working scenarios)

**Note**: Research Assistant scenario has known timeout issues that affect reliability. Most scenarios demonstrate the full cost savings pattern successfully.

## Memory and Learning

### Trace Evolution

```
Initial execution:
  confidence: 0.75
  cost: $0.50
  time: 15s

After 5 successful uses:
  confidence: 0.95
  cost: $0.00 (Follower mode)
  time: 0.5s
```

### Cross-Project Patterns

The demo shows how patterns learned in one project can benefit others:
- Reusable agent templates
- Common task patterns
- Optimization insights
- Anti-pattern detection

## Running the Demo

### Interactive Menu

```bash
python demo_main.py
```

You'll see:
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘   LLM OS - Demo Application (Phase 2.5)  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Select a demo scenario:
1. Data Processing Pipeline
2. Code Generation Workflow
3. Research Assistant
4. DevOps Automation
5. Cross-Project Learning Demo
6. Run All Scenarios
7. Exit

Choice (1-7):
```

### Command Line Options

```bash
# Run specific scenario
python demo_main.py --scenario data-pipeline

# Run all scenarios
python demo_main.py --all

# Set custom budget
python demo_main.py --budget 20.0

# Enable verbose mode
python demo_main.py --verbose

# Use specific project
python demo_main.py --project my_project
```

## Expected Outputs

After running demos, you'll find:

```
output/
â”œâ”€â”€ projects/
â”‚   â”œâ”€â”€ data_pipeline_demo/
â”‚   â”‚   â”œâ”€â”€ components/         # Project agents
â”‚   â”‚   â”œâ”€â”€ memory/            # Project traces
â”‚   â”‚   â”œâ”€â”€ output/            # Generated files
â”‚   â”‚   â””â”€â”€ state/             # Execution state
â”‚   â””â”€â”€ research_demo/
â”‚       â””â”€â”€ ...
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ data_analysis_report.md
â”‚   â”œâ”€â”€ research_summary.md
â”‚   â””â”€â”€ cost_analysis.md
â””â”€â”€ traces/
    â””â”€â”€ trace_*.md             # Execution traces
```

## Key Concepts Demonstrated

### 1. Token Economy
Every demo shows:
- Initial budget allocation
- Cost per execution
- Budget monitoring
- Savings via Follower mode

### 2. Memory Hierarchy
- **L1**: Context window (active conversation)
- **L2**: Short-term memory (session logs)
- **L3**: Procedural memory (execution traces - Markdown)
- **L4**: Semantic memory (facts and insights - files)

### 3. Plugin Architecture
Custom tools can be added to any demo:
```python
@llm_tool("analyze_csv", "Analyze CSV file", {"path": str})
async def analyze_csv(path: str):
    # Custom analysis logic
    return {"insights": [...]}
```

### 4. Event-Driven Architecture
All demos use the event bus:
- Tool execution events
- LLM output events
- Budget events
- State change events

## Troubleshooting

### "Claude Agent SDK not installed"
```bash
pip install claude-agent-sdk
```

### "ANTHROPIC_API_KEY not found"
```bash
export ANTHROPIC_API_KEY="your-key-here"
```

### "Low battery error"
Increase budget:
```bash
python demo_main.py --budget 50.0
```

### "Permission denied on workspace"
```bash
chmod -R 755 ../llmos/workspace/
```

### "Delegation timed out after 300.0s" (Research Assistant)

This is a **known issue** with the Research Assistant scenario:
- Some agent delegations timeout after 5 minutes
- WebSearch tool may not be available in delegated agents
- System continues with remaining steps and generates partial results

**Workaround**: Use Data Pipeline scenario for reliable multi-agent demonstration.

**Status**: Under investigation for Phase 2.6 improvements.

### "No messages for 60s - stopping delegation"

This warning appears during the Research scenario when delegated agents take too long to respond. It's informational and part of the timeout handling. The system will continue with remaining steps.

### General Performance Notes

- **Code Generation**: âœ… Fully working, best first demo
- **Cost Optimization**: âœ… Fully working, shows dramatic savings
- **Data Pipeline**: âœ… Fully working, recommended for multi-agent demo
- **Research Assistant**: âš ï¸ Has timeout issues, demonstrates pattern but not fully functional
- **SDK Hooks**: âœ… Fully working, shows all Phase 2.5 features
- **DevOps**: âœ… Fully working
- **Cross-Project**: âœ… Fully working

## Next Steps

After running the demo:

1. **Explore the Traces**: Check `output/traces/` to see captured execution patterns
2. **Modify Scenarios**: Edit scenarios in `scenarios/` to customize for your use case
3. **Create Custom Agents**: Add new agents in `agents/`
4. **Add Custom Tools**: Create plugins for domain-specific functionality
5. **Run Real Workflows**: Apply llmos to your actual projects

## Documentation

- Main docs: `../llmos/README.md`
- Architecture: `../llmos/ARCHITECTURE.md`
- Getting Started: `../llmos/GETTING_STARTED.md`
- Examples: `../llmos/examples/multi_agent_example.py`

## Philosophy

This demo embodies the llmos philosophy:

1. **Learn Once, Execute Infinitely**: First run is expensive, repeats are free
2. **Token-Aware**: Every decision considers cost
3. **Memory as Code**: Execution traces are executable knowledge
4. **Event-Driven**: Async, non-blocking architecture
5. **Plugin-Based**: Domain-agnostic core
6. **Multi-Agent Ready**: Orchestration for complex tasks

## Contributing

To add new scenarios:

1. Create file in `scenarios/`
2. Implement scenario class with `run()` method
3. Add to menu in `demo_main.py`
4. Document in this README

## License

Apache 2.0 (same as llmos)

---

Built with LLM OS - The Generic LLM Operating System
Phase 2.5: Multi-Agent + SDK Hooks + Streaming + Advanced Options
